# -*- coding: utf-8 -*-
"""EEG_Sixian.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Iw2CkggTfw4okd-hPMQGGiD7HixAJPBn
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
# %cd /content/drive/MyDrive/eec247_final_project/

import torch
import numpy as np
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import os,stat
import copy
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import itertools

import numpy as np

import torch
from torch import nn
from torch.nn import init
from torch.nn.functional import elu


def transpose_time_to_spat(x):
    """Swap time and spatial dimensions.
    Returns
    -------
    x: torch.Tensor
        tensor in which last and first dimensions are swapped
    """
    return x.permute(0, 3, 2, 1)

def squeeze_final_output(x):
    """Removes empty dimension at end and potentially removes empty time
     dimension. It does  not just use squeeze as we never want to remove
     first dimension.
    Returns
    -------
    x: torch.Tensor
        squeezed tensor
    """

    assert x.size()[3] == 1
    x = x[:, :, :, 0]
    if x.size()[2] == 1:
        x = x[:, :, 0]
    return x

class Ensure4d(nn.Module):
    def forward(self, x):
        while(len(x.shape) < 4):
            x = x.unsqueeze(-1)
        return x


class Expression(nn.Module):
    """Compute given expression on forward pass.
    Parameters
    ----------
    expression_fn : callable
        Should accept variable number of objects of type
        `torch.autograd.Variable` to compute its output.
    """

    def __init__(self, expression_fn):
        super(Expression, self).__init__()
        self.expression_fn = expression_fn

    def forward(self, *x):
        return self.expression_fn(*x)

    def __repr__(self):
        if hasattr(self.expression_fn, "func") and hasattr(
            self.expression_fn, "kwargs"
        ):
            expression_str = "{:s} {:s}".format(
                self.expression_fn.func.__name__, str(self.expression_fn.kwargs)
            )
        elif hasattr(self.expression_fn, "__name__"):
            expression_str = self.expression_fn.__name__
        else:
            expression_str = repr(self.expression_fn)
        return (
            self.__class__.__name__ +
            "(expression=%s) " % expression_str
        )


class AvgPool2dWithConv(nn.Module):
    """
    Compute average pooling using a convolution, to have the dilation parameter.
    Parameters
    ----------
    kernel_size: (int,int)
        Size of the pooling region.
    stride: (int,int)
        Stride of the pooling operation.
    dilation: int or (int,int)
        Dilation applied to the pooling filter.
    padding: int or (int,int)
        Padding applied before the pooling operation.
    """

    def __init__(self, kernel_size, stride, dilation=1, padding=0):
        super(AvgPool2dWithConv, self).__init__()
        self.kernel_size = kernel_size
        self.stride = stride
        self.dilation = dilation
        self.padding = padding
        # don't name them "weights" to
        # make sure these are not accidentally used by some procedure
        # that initializes parameters or something
        self._pool_weights = None

    def forward(self, x):
        # Create weights for the convolution on demand:
        # size or type of x changed...
        in_channels = x.size()[1]
        weight_shape = (
            in_channels,
            1,
            self.kernel_size[0],
            self.kernel_size[1],
        )
        if self._pool_weights is None or (
            (tuple(self._pool_weights.size()) != tuple(weight_shape)) or
            (self._pool_weights.is_cuda != x.is_cuda) or
            (self._pool_weights.data.type() != x.data.type())
        ):
            n_pool = np.prod(self.kernel_size)
            weights = np_to_th(
                np.ones(weight_shape, dtype=np.float32) / float(n_pool)
            )
            weights = weights.type_as(x)
            if x.is_cuda:
                weights = weights.cuda()
            self._pool_weights = weights

        pooled = F.conv2d(
            x,
            self._pool_weights,
            bias=None,
            stride=self.stride,
            dilation=self.dilation,
            padding=self.padding,
            groups=in_channels,
        )
        return pooled

class EEGResNet(nn.Sequential):
    """Residual Network for EEG.
    XXX missing reference
    Parameters
    ----------
    in_chans : int
        XXX
    """
    def __init__(self,
                 in_chans,
                 n_classes,
                 input_window_samples,
                 final_pool_length,
                 n_first_filters,
                 n_layers_per_block=2,
                 first_filter_length=3,
                 nonlinearity=elu,
                 split_first_layer=True,
                 batch_norm_alpha=0.1,
                 batch_norm_epsilon=1e-4,
                 conv_weight_init_fn=lambda w: init.kaiming_normal_(w, a=0)):
        super().__init__()
        self.in_chans = in_chans
        self.n_classes = n_classes
        self.input_window_samples = input_window_samples
        if final_pool_length == 'auto':
            assert input_window_samples is not None
        assert first_filter_length % 2 == 1
        self.final_pool_length = final_pool_length
        self.n_first_filters = n_first_filters
        self.n_layers_per_block = n_layers_per_block
        self.first_filter_length = first_filter_length
        self.nonlinearity = nonlinearity
        self.split_first_layer = split_first_layer
        self.batch_norm_alpha = batch_norm_alpha
        self.batch_norm_epsilon = batch_norm_epsilon
        self.conv_weight_init_fn = conv_weight_init_fn

        self.add_module("ensuredims", Ensure4d())
        if self.split_first_layer:
            self.add_module('dimshuffle', Expression(transpose_time_to_spat))
            self.add_module('conv_time', nn.Conv2d(1, self.n_first_filters,
                                                   (self.first_filter_length, 1),
                                                   stride=1,
                                                   padding=(self.first_filter_length // 2, 0)))
            self.add_module('conv_spat',
                            nn.Conv2d(self.n_first_filters, self.n_first_filters,
                                      (1, self.in_chans),
                                      stride=(1, 1),
                                      bias=False))
        else:
            self.add_module('conv_time',
                            nn.Conv2d(self.in_chans, self.n_first_filters,
                                      (self.first_filter_length, 1),
                                      stride=(1, 1),
                                      padding=(self.first_filter_length // 2, 0),
                                      bias=False,))
        n_filters_conv = self.n_first_filters
        self.add_module('bnorm',
                        nn.BatchNorm2d(n_filters_conv,
                                       momentum=self.batch_norm_alpha,
                                       affine=True,
                                       eps=1e-5),)
        self.add_module('conv_nonlin', Expression(self.nonlinearity))
        cur_dilation = np.array([1, 1])
        n_cur_filters = n_filters_conv
        i_block = 1
        for i_layer in range(self.n_layers_per_block):
            self.add_module('res_{:d}_{:d}'.format(i_block, i_layer),
                            _ResidualBlock(n_cur_filters, n_cur_filters,
                                           dilation=cur_dilation))
        i_block += 1
        cur_dilation[0] *= 2
        n_out_filters = int(2 * n_cur_filters)
        self.add_module('res_{:d}_{:d}'.format(i_block, 0),
                        _ResidualBlock(n_cur_filters, n_out_filters,
                                       dilation=cur_dilation,))
        n_cur_filters = n_out_filters
        for i_layer in range(1, self.n_layers_per_block):
            self.add_module('res_{:d}_{:d}'.format(i_block, i_layer),
                            _ResidualBlock(n_cur_filters, n_cur_filters,
                                           dilation=cur_dilation))

        i_block += 1
        cur_dilation[0] *= 2
        n_out_filters = int(1.5 * n_cur_filters)
        self.add_module('res_{:d}_{:d}'.format(i_block, 0),
                        _ResidualBlock(n_cur_filters, n_out_filters,
                                       dilation=cur_dilation,))
        n_cur_filters = n_out_filters
        for i_layer in range(1, self.n_layers_per_block):
            self.add_module('res_{:d}_{:d}'.format(i_block, i_layer),
                            _ResidualBlock(n_cur_filters, n_cur_filters,
                                           dilation=cur_dilation))

        i_block += 1
        cur_dilation[0] *= 2
        self.add_module('res_{:d}_{:d}'.format(i_block, 0),
                        _ResidualBlock(n_cur_filters, n_cur_filters,
                                       dilation=cur_dilation,))
        for i_layer in range(1, self.n_layers_per_block):
            self.add_module('res_{:d}_{:d}'.format(i_block, i_layer),
                            _ResidualBlock(n_cur_filters, n_cur_filters,
                                           dilation=cur_dilation))

        i_block += 1
        cur_dilation[0] *= 2
        self.add_module('res_{:d}_{:d}'.format(i_block, 0),
                        _ResidualBlock(n_cur_filters, n_cur_filters,
                                       dilation=cur_dilation,))
        for i_layer in range(1, self.n_layers_per_block):
            self.add_module('res_{:d}_{:d}'.format(i_block, i_layer),
                            _ResidualBlock(n_cur_filters, n_cur_filters,
                                           dilation=cur_dilation))

        i_block += 1
        cur_dilation[0] *= 2
        self.add_module('res_{:d}_{:d}'.format(i_block, 0),
                        _ResidualBlock(n_cur_filters, n_cur_filters,
                                       dilation=cur_dilation,))
        for i_layer in range(1, self.n_layers_per_block):
            self.add_module('res_{:d}_{:d}'.format(i_block, i_layer),
                            _ResidualBlock(n_cur_filters, n_cur_filters,
                                           dilation=cur_dilation))
        i_block += 1
        cur_dilation[0] *= 2
        self.add_module('res_{:d}_{:d}'.format(i_block, 0),
                        _ResidualBlock(n_cur_filters, n_cur_filters,
                                       dilation=cur_dilation,))
        for i_layer in range(1, self.n_layers_per_block):
            self.add_module('res_{:d}_{:d}'.format(i_block, i_layer),
                            _ResidualBlock(n_cur_filters, n_cur_filters,
                                           dilation=cur_dilation))

        self.eval()
        if self.final_pool_length == 'auto':
            self.add_module('mean_pool', nn.AdaptiveAvgPool2d((1, 1)))
        else:
            pool_dilation = int(cur_dilation[0]), int(cur_dilation[1])
            self.add_module('mean_pool', AvgPool2dWithConv(
                (self.final_pool_length, 1), (1, 1),
                dilation=pool_dilation))
        self.add_module('conv_classifier',
                        nn.Conv2d(n_cur_filters, self.n_classes,
                                  (1, 1), bias=True))
        self.add_module('softmax', nn.LogSoftmax(dim=1))
        self.add_module('squeeze', Expression(squeeze_final_output))

        # Initialize all weights
        self.apply(lambda module: _weights_init(module, self.conv_weight_init_fn))

        # Start in eval mode
        self.eval()


def _weights_init(module, conv_weight_init_fn):
    """
    initialize weights
    """
    classname = module.__class__.__name__
    if 'Conv' in classname and classname != "AvgPool2dWithConv":
        conv_weight_init_fn(module.weight)
        if module.bias is not None:
            init.constant_(module.bias, 0)
    elif 'BatchNorm' in classname:
        init.constant_(module.weight, 1)
        init.constant_(module.bias, 0)


class _ResidualBlock(nn.Module):
    """
    create a residual learning building block with two stacked 3x3 convlayers as in paper
    """
    def __init__(self, in_filters,
                 out_num_filters,
                 dilation,
                 filter_time_length=3,
                 nonlinearity=elu,
                 batch_norm_alpha=0.1, batch_norm_epsilon=1e-4):
        super(_ResidualBlock, self).__init__()
        time_padding = int((filter_time_length - 1) * dilation[0])
        assert time_padding % 2 == 0
        time_padding = int(time_padding // 2)
        dilation = (int(dilation[0]), int(dilation[1]))
        assert (out_num_filters - in_filters) % 2 == 0, (
            "Need even number of extra channels in order to be able to "
            "pad correctly")
        self.n_pad_chans = out_num_filters - in_filters

        self.conv_1 = nn.Conv2d(
            in_filters, out_num_filters, (filter_time_length, 1), stride=(1, 1),
            dilation=dilation,
            padding=(time_padding, 0))
        self.bn1 = nn.BatchNorm2d(
            out_num_filters, momentum=batch_norm_alpha, affine=True,
            eps=batch_norm_epsilon)
        self.conv_2 = nn.Conv2d(
            out_num_filters, out_num_filters, (filter_time_length, 1), stride=(1, 1),
            dilation=dilation,
            padding=(time_padding, 0))
        self.bn2 = nn.BatchNorm2d(
            out_num_filters, momentum=batch_norm_alpha,
            affine=True, eps=batch_norm_epsilon)
        # also see https://mail.google.com/mail/u/0/#search/ilya+joos/1576137dd34c3127
        # for resnet options as ilya used them
        self.nonlinearity = nonlinearity

    def forward(self, x):
        stack_1 = self.nonlinearity(self.bn1(self.conv_1(x)))
        stack_2 = self.bn2(self.conv_2(stack_1))  # next nonlin after sum
        if self.n_pad_chans != 0:
            zeros_for_padding = torch.autograd.Variable(
                torch.zeros(x.size()[0], self.n_pad_chans // 2, x.size()[2], x.size()[3]))
            if x.is_cuda:
                zeros_for_padding = zeros_for_padding.cuda()
            x = torch.cat((zeros_for_padding, x, zeros_for_padding), dim=1)
        out = self.nonlinearity(x + stack_2)
        return out

class ConvNets(nn.Module):
    def __init__(self, pad=0):
        super(ConvNets, self).__init__()
        self.pad = pad
        self.conv1_time = nn.Conv2d(1, 25, kernel_size=(10,1), stride=1, padding=(5,0))
        self.conv1_space = nn.Conv2d(25,25,kernel_size=(1,22),stride=1,padding=0)
        self.pool1 = nn.MaxPool2d(kernel_size=(3,1),stride=(3,1),padding=(1,0))
        self.bn1 = nn.BatchNorm2d(25)
        self.drop1 = nn.Dropout()        
        self.conv2 = nn.Conv2d(25, 50, kernel_size=(10,1), stride=1, padding=(5,0))
        self.pool2 = nn.MaxPool2d(kernel_size=(3,1),stride=(3,1),padding=(1,0))
        self.bn2 = nn.BatchNorm2d(50)
        self.drop2 = nn.Dropout()        
        self.conv3 = nn.Conv2d(50, 100, kernel_size=(10,1), stride=1, padding=(5,0))
        self.pool3 = nn.MaxPool2d(kernel_size=(3,1),stride=(3,1),padding=(1,0))
        self.bn3 = nn.BatchNorm2d(100)
        self.drop3 = nn.Dropout()        
        self.conv4 = nn.Conv2d(100,200, kernel_size=(10,1), stride=1, padding=(5,0))
        self.pool4 = nn.MaxPool2d(kernel_size=(3,1),stride=(3,1),padding=(1,0))
        self.bn4 = nn.BatchNorm2d(200)
        self.drop4 = nn.Dropout()        
        self.fc1 = nn.Linear(800, 4)
        self.softmax = nn.LogSoftmax(dim=1)
    def forward(self, x):
        x = self.conv1_time(x)
        #print(x.shape)
        x = F.elu(self.conv1_space(x))
        #print(x.shape)
        x = self.pool1(x)
        #print(x.shape)
        x = self.bn1(x)
        x = self.drop1(x)
        #print(x.shape)
        x = F.elu(self.conv2(x))
        #print(x.shape)
        x = self.pool2(x)
        #print(x.shape)
        x = self.bn2(x)
        x = self.drop2(x)
        #print(x.shape)
        x = F.elu(self.conv3(x))
        #print(x.shape)
        x = self.pool3(x)
        #print(x.shape)
        x = self.bn3(x)
        x = self.drop3(x)
        #print(x.shape)
        x = F.elu(self.conv4(x))
        #print(x.shape)
        x = self.pool4(x)
        #print(x.shape)
        x = self.bn4(x)
        x = self.drop4(x)
        
        x = x.view(-1, 800)
        x = self.fc1(x)
        
        return self.softmax(x)

def data_prep(X,y,sub_sample,average,noise):
    total_X = None
    total_y = None
    # Trimming the data (sample,22,1000) -> (sample,22,500)
    X = X[:,:,0:500]
    #print('Shape of X after trimming:',X.shape)
    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)
    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)
    total_X = X_max
    total_y = y
    #print('Shape of X after maxpooling:',total_X.shape)
    # Averaging + noise 
    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)
    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)
    total_X = np.vstack((total_X, X_average))
    total_y = np.hstack((total_y, y))
    #print('Shape of X after averaging+noise and concatenating:',total_X.shape)
    # Subsampling
    for i in range(sub_sample):
        X_subsample = X[:, :, i::sub_sample] + (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)
        total_X = np.vstack((total_X, X_subsample))
        total_y = np.hstack((total_y, y))
    #print('Shape of X after subsampling and concatenating:',total_X.shape)
    return total_X,total_y

seed = 0
epoch=200
batch=32
cpu=False
save_dir = "./training_data/"
cross=True

global args, best_prec1
seed = 0
torch.manual_seed(seed)
np.random.seed(seed)
torch.backends.cudnn.deterministic=True
train_epoch = int(epoch)
batch = batch

x_test = np.load("X_test.npy")
y_test = np.load("y_test.npy")
person_train_valid = np.load("person_train_valid.npy")
X_train_valid = np.load("X_train_valid.npy")
y_train_valid = np.load("y_train_valid.npy")
person_test = np.load("person_test.npy")
    
y_train_valid -= 769
y_test -= 769
X_train=[]
Y_train=[]
X_test=[]
Y_test=[]
for i in range(9):
  indices = [index for index, element in enumerate(person_train_valid[:,]) if element == i]
  X_train.append(X_train_valid[indices])
  Y_train.append(y_train_valid[indices])
for i in range(9):
  indices = [index for index, element in enumerate(person_test[:,]) if element == i]
  X_test.append(x_test[indices])
  Y_test.append(y_test[indices])
#currently only train for subject 0
if cross:
  X_train=np.array(list(itertools.chain.from_iterable(X_train)))
  Y_train=np.array(list(itertools.chain.from_iterable(Y_train)))
  X_test=np.array(list(itertools.chain.from_iterable(X_test)))
  Y_test=np.array(list(itertools.chain.from_iterable(Y_test)))
else:
  X_train=X_train[0]
  Y_train=Y_train[0]
  X_test=X_test[0]
  Y_test=Y_test[0]
## Visualizing the data
X_train_valid = X_train
y_train_valid = Y_train
y_test= Y_test
X_train_valid_prep,y_train_valid_prep = data_prep(X_train_valid,y_train_valid,2,2,True)
X_test_prep,y_test_prep = data_prep(X_test,y_test,2,2,True)
if cross:
  ind_valid = np.random.choice(8460, 1500, replace=False)
  ind_train = np.array(list(set(range(8460)).difference(set(ind_valid))))
else:
  ind_valid = np.random.choice(948, 200, replace=False)
  ind_train = np.array(list(set(range(948)).difference(set(ind_valid))))
  # Creating the training and validation sets using the generated indices
(x_train, x_valid) = X_train_valid_prep[ind_train], X_train_valid_prep[ind_valid] 
(y_train, y_valid) = y_train_valid_prep[ind_train], y_train_valid_prep[ind_valid]
x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)
x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)
x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)
print('Shape of training set after adding width info:',x_train.shape)
print('Shape of validation set after adding width info:',x_valid.shape)
print('Shape of test set after adding width info:',x_test.shape)
# Reshaping the training and validation dataset
x_train = np.swapaxes(x_train, 1,3)
x_valid = np.swapaxes(x_valid, 1,3)
x_test = np.swapaxes(x_test, 1,3)
print('Shape of training set after dimension reshaping:',x_train.shape)
print('Shape of validation set after dimension reshaping:',x_valid.shape)
print('Shape of test set after dimension reshaping:',x_test.shape)
if not cpu:
  tensor_train_x = torch.Tensor(x_train).cuda()
  tensor_train_y = torch.Tensor(y_train).cuda()
  tensor_valid_x = torch.Tensor(x_valid).cuda()
  tensor_valid_y = torch.Tensor(y_valid).cuda()
  tensor_test_y = torch.Tensor(y_test_prep).cuda()
  tensor_test_x = torch.Tensor(x_test).cuda()
else:
  tensor_train_x = torch.Tensor(x_train)
  tensor_train_y = torch.Tensor(y_train)
  tensor_valid_x = torch.Tensor(x_valid)
  tensor_valid_y = torch.Tensor(y_valid)
  tensor_test_y = torch.Tensor(y_test_prep)
  tensor_test_x = torch.Tensor(x_test)
train_dataset = TensorDataset(tensor_train_x,tensor_train_y)
valid_dataset = TensorDataset(tensor_valid_x,tensor_valid_y)
test_dataset = TensorDataset(tensor_test_x,tensor_test_y)
trainloader = DataLoader(train_dataset,batch_size=batch)
validloader = DataLoader(valid_dataset)
testloader = DataLoader(test_dataset)

device = 0

if not os.path.exists(save_dir):
  os.makedirs(save_dir)
net = EEGResNet(in_chans = 8, n_classes = 1, input_window_samples = 4096, final_pool_length = 'auto', n_first_filters = 2)
save_file = save_dir+'ConvNet'
save_file += '.pt'

if not cpu:
  torch.cuda.empty_cache()
  net.cuda(device)
  testdata=torch.tensor([2,2,2,2])
  testdata.cuda()

learning_rate = 0.0005
criterion = nn.CrossEntropyLoss()
lambda1 = lambda2 = lambda epoch: 0.99 ** epoch
optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.0001)
scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda1)
running_loss = 0.0
best_prec = 0
net_best=copy.deepcopy(net)
net.train()

for e in range(train_epoch):
  for i, data in enumerate(trainloader, 0):
    inputs, labels = data
    labels = labels.type(torch.int64)
    # zero the parameter gradients
    optimizer.zero_grad()
    # forward + backward + optimize
    outputs = net(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    # print statistics
    running_loss += loss.item()
  print(f'[{e + 1}] loss: {running_loss :.3f}', end=', ')
  running_loss = 0.0
  correct =0
  total=0
  net.eval()
  for j,v in enumerate(trainloader,0):
    test,label = v
    output=net(test)
    _, predicted = torch.max(output.data, 1)
    total += label.size(0)
    correct += (predicted == label).sum().item()
  val_prec = correct/total
  print(f'Training accuracy: {val_prec*100.} %',end=', ')
  correct =0
  total=0
  for j,v in enumerate(validloader,0):
    test,label = v
    output=net(test)
    _, predicted = torch.max(output.data, 1)
    total += label.size(0)
    correct += (predicted == label).sum().item()
  val_prec = correct/total
  print(f'Validation accuracy: {val_prec*100.} %')
  is_best = val_prec > best_prec
  best_prec = max(val_prec, best_prec)
  if is_best:
    torch.save(net,save_file)
    net_best = copy.deepcopy(net)
  net.train()
  scheduler.step()
correct =0
total=0
net_best.eval()
for j,v in enumerate(testloader,0):
  test,label = v
  output=net_best(test)
  _, predicted = torch.max(output.data, 1)
  total += label.size(0)
  correct += (predicted == label).sum().item()
val_prec = correct/total
print(f'Test accuracy: {val_prec*100.} %')
print('Finished Training')

